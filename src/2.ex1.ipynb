{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/phuongnm/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|██████████| 3/3 [00:00<00:00, 133.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def split_tokens(sentence):                             \n",
    "    return [w for w in re.split(r\" +\",  re.sub(r\"[^a-z@# ]\", \"\", sentence.lower()))]   \n",
    "\n",
    "dataset = load_dataset('emotion')\n",
    "train_data = dataset['train']\n",
    "all_words = []\n",
    "all_labels = []\n",
    "for sample in train_data:\n",
    "    all_words+= split_tokens(sample['text']) \n",
    "    all_labels.append(sample['label'])\n",
    "\n",
    "# build vocab - using vocab object of torchtext \n",
    "counter = Counter(all_words)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "my_vocab = vocab(OrderedDict(sorted_by_freq_tuples), specials=['<pad>','<unk>'])\n",
    "my_vocab.set_default_index(my_vocab['<unk>'])\n",
    "\n",
    "# count label \n",
    "num_labels = len(set(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def convert_sentence_to_ids(sentence, vocab):    \n",
    "    word_ids = None\n",
    "    # ===================================\n",
    "    # REQUIREMENT:\n",
    "    # - split sentence to tokens using `split_tokens` defined above\n",
    "    # (tips: split sentence to list of words, then feed to the vocab to get list of id) \n",
    "    # ===================================\n",
    "    # - PUSH YOUR CODE IN HERE, can not modify any code in outside this range. \n",
    "    \n",
    "\n",
    "    \n",
    "    # ===================================\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "def get_max_sentence_length_in_batch(batch_input_ids): \n",
    "    # ===================================\n",
    "    # REQUIREMENT:\n",
    "    # - find and return the MAXIMUM length (number of word) of each sample (sentence) in a batch.\n",
    "    # ===================================\n",
    "    # - PUSH YOUR CODE IN HERE, can not modify any code in outside this range. \n",
    "     \n",
    "\n",
    "    # ===================================\n",
    "    return max_sentence_length\n",
    "\n",
    "\n",
    "def add_padding(batch_input_ids, padding_id):\n",
    "    max_sample_len_in_batch = get_max_sentence_length_in_batch(batch_input_ids=batch_input_ids)\n",
    "\n",
    "    # ===================================\n",
    "    # REQUIREMENT:\n",
    "    # - batch data contains many sentence having difference number of words. To train a deep learning model\n",
    "    #   we need to convert it to tensor which have the same length for all sentences. \n",
    "    # - We need to add padding into each sentence (sample) in a batch. \n",
    "    # - for example: a batch contains [[1,2,3,4],[6,7,8],[9]] ==(after padding 0)==> [[1,2,3,4],[6,7,8,0],[9,0,0,0]]\n",
    "    # (tips: each sample, calculate the number of padding tokens need to add to get max_sample_len_in_batch) \n",
    "    # ===================================\n",
    "    # - PUSH YOUR CODE IN HERE, can not modify any code in outside this range.  \n",
    "\n",
    "\n",
    "    # ===================================\n",
    "    return padded_word_ids\n",
    "\n",
    "\n",
    "class BatchPreprocessor(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs = []\n",
    "        masks = []\n",
    "\n",
    "        # covert text to number \n",
    "        for sample in batch:\n",
    "            word_ids = convert_sentence_to_ids(sample['text'], self.vocab)\n",
    "            inputs.append(word_ids)\n",
    "        \n",
    "        # padding to create a tensor input - make all sentence having the same length \n",
    "        padding_id = self.vocab[\"<pad>\"]\n",
    "        padded_batch = add_padding(batch_input_ids=inputs, padding_id=padding_id)\n",
    "\n",
    "        # label processing \n",
    "        labels = []\n",
    "        for sample in batch:\n",
    "            label = sample['label']\n",
    "            labels.append(int(label))\n",
    "\n",
    "        # make a tensor \n",
    "        inputs = torch.LongTensor(padded_batch)\n",
    "\n",
    "        # make mask flag tensor\n",
    "        masks = inputs == padding_id\n",
    "\n",
    "        return (inputs, torch.LongTensor(labels), torch.BoolTensor(masks)) \n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# dataset_example should support operator index_selection for create the data_loader object\n",
    "test_loader = DataLoader(dataset['test'], batch_size=batch_size, collate_fn=BatchPreprocessor(my_vocab), shuffle=True)\n",
    "for e in test_loader:\n",
    "    print('First epoch data:')\n",
    "    print('input data\\n', e[0])\n",
    "    print('label data\\n',e[1])\n",
    "    print('padding mask data\\n',e[2])\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correct output look like\n",
    "\n",
    "```python\n",
    "First epoch data:\n",
    "input data\n",
    " tensor([[    2,    21,   700,     5,   113,    54,    13,    65,    28,    14,\n",
    "            49,     2,   411,    71,    10,    11,   321,   173,    19,    13,\n",
    "            99,   145,    41,   848,     4,     2,    40,   118,   165,     3,\n",
    "            77,   385,  1112,     5,    64,    37,    62,    44,    11,   173],\n",
    "        [    2,     3,    15,   714,     4,   154,     5,   644,    54,    22,\n",
    "            61,   200,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [    2,   252,    13,   818,    43,   140,     9,     2,   141,   586,\n",
    "            30,  1015,  1217,     4,    20,  1856,   615,    11,    59,     5,\n",
    "          1873,    11,  2544,     5,   575,    12,    19,    61,   100,    12,\n",
    "             6,  1226,   123,   272,     0,     0,     0,     0,     0,     0],\n",
    "        [    2,   747,     5,    39,    15,    45,     1,    77, 10083,     4,\n",
    "            60,     9, 10083,  1658,     2,     3,     9,     6,   134,   591,\n",
    "          6395,     2,    65,    14,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [    2,   145,    48,     3,    23, 10265,    23,   679,    12,    78,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
    "label data\n",
    " tensor([0, 1, 5, 1, 0])\n",
    "padding mask data\n",
    " tensor([[False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
    "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
    "        [False, False, False, False, False, False, False, False, False, False,\n",
    "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3602b4515afb2d87a870b61c65d7b658117eca8f37f64d20593019ba04f7019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
