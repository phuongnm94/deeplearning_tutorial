{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 4., 2., 9., 1.],\n",
      "        [4., 9., 2., 7., 8.],\n",
      "        [1., 8., 3., 6., 0.]])\n",
      "tensor([[ 58.],\n",
      "        [104.],\n",
      "        [ 56.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    " \n",
    "# =====================\n",
    "# generate data\n",
    "def gen_fake_data(hidden_weight, hidden_bias):\n",
    "    def rand_5_numbers_from1to10():\n",
    "        return np.random.permutation(10)[:5]\n",
    "\n",
    "    x_train = torch.Tensor([rand_5_numbers_from1to10() for i in range(60)]) \n",
    "    x_test = torch.Tensor([rand_5_numbers_from1to10() for i in range(60)]) \n",
    "    \n",
    "    hidden_weight = torch.Tensor(hidden_weight).unsqueeze(-1) \n",
    "    hidden_bias = torch.Tensor([hidden_bias]) \n",
    "\n",
    "    y_train = torch.mm(x_train, hidden_weight) + hidden_bias\n",
    "    y_test = torch.mm(x_test, hidden_weight) + hidden_bias\n",
    "    return x_train, y_train, x_test, y_test\n",
    "     \n",
    "# gen training data sample \n",
    "x_train, y_train, x_test, y_test = gen_fake_data(hidden_weight=[1,3,2,4,5], hidden_bias=1)\n",
    "print(x_train[:3])\n",
    "print(y_train[:3])\n",
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataloader\n",
    "# dataset_example should support operator index_selection for create the data_loader object\n",
    "train_loader = DataLoader([(x_train[i], y_train[i]) for i in range(x_train.shape[0])], batch_size=10, collate_fn=None, shuffle=True)\n",
    "test_loader = DataLoader([(x_test[i], y_test[i]) for i in range(x_test.shape[0])], batch_size=50, collate_fn=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model \n",
    "class ModelLinear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.rand((input_size, output_size),requires_grad=True, device=torch.device('cuda:0'))\n",
    "        self.bias = torch.rand(output_size, requires_grad=True, device=torch.device('cuda:0') )\n",
    "\n",
    "    def __call__(self, input_matrix):\n",
    "        return torch.mm(input_matrix, self.weight) + self.bias\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [self.bias, self.weight]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice part \n",
    "- train a model to find hidden \"weights and bias\" from training data.\n",
    "- **TODO**, practice with learning rate equal to 0.005, 0.02 0.01 to see the result how the model work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/batch:  0 0  loss:  38.480247497558594\n",
      "epoch/batch:  1 1  loss:  35.1999626159668\n",
      "epoch/batch:  2 2  loss:  27.283655166625977\n",
      "epoch/batch:  3 3  loss:  10.864322662353516\n",
      "epoch/batch:  4 4  loss:  6.0600738525390625\n",
      "epoch/batch:  5 5  loss:  3.509241819381714\n",
      "epoch/batch:  6 6  loss:  3.6849076747894287\n",
      "epoch/batch:  7 7  loss:  2.16418719291687\n",
      "epoch/batch:  8 8  loss:  1.2757031917572021\n",
      "epoch/batch:  9 9  loss:  0.42720356583595276\n",
      "epoch/batch:  10 10  loss:  0.6071180105209351\n",
      "epoch/batch:  11 11  loss:  0.2020016461610794\n",
      "epoch/batch:  12 12  loss:  0.17425628006458282\n",
      "epoch/batch:  13 13  loss:  0.09398400038480759\n",
      "epoch/batch:  14 14  loss:  0.10839567333459854\n",
      "epoch/batch:  15 15  loss:  0.11964117735624313\n",
      "epoch/batch:  16 16  loss:  0.07217350602149963\n",
      "epoch/batch:  17 17  loss:  0.03018859401345253\n",
      "epoch/batch:  18 18  loss:  0.014126230962574482\n",
      "epoch/batch:  19 19  loss:  0.015111957676708698\n",
      "epoch/batch:  20 20  loss:  0.012493959628045559\n",
      "epoch/batch:  21 21  loss:  0.0065611908212304115\n",
      "epoch/batch:  22 22  loss:  0.0034732960630208254\n",
      "epoch/batch:  23 23  loss:  0.008055384270846844\n",
      "epoch/batch:  24 24  loss:  0.007607740815728903\n",
      "epoch/batch:  25 25  loss:  0.004554664250463247\n",
      "epoch/batch:  26 26  loss:  0.00489302072674036\n",
      "epoch/batch:  27 27  loss:  0.0032305873464792967\n",
      "epoch/batch:  28 28  loss:  0.006974421441555023\n",
      "epoch/batch:  29 29  loss:  0.0015997423324733973\n",
      "epoch/batch:  30 30  loss:  0.0035560179967433214\n",
      "epoch/batch:  31 31  loss:  0.006199810188263655\n",
      "epoch/batch:  32 32  loss:  0.003629593877121806\n",
      "epoch/batch:  33 33  loss:  0.0033757505007088184\n",
      "epoch/batch:  34 34  loss:  0.004052857868373394\n",
      "epoch/batch:  35 35  loss:  0.005095717962831259\n",
      "epoch/batch:  36 36  loss:  0.0032660458236932755\n",
      "epoch/batch:  37 37  loss:  0.0035170658957213163\n",
      "epoch/batch:  38 38  loss:  0.0036950677167624235\n",
      "epoch/batch:  39 39  loss:  0.002022859640419483\n",
      "epoch/batch:  40 40  loss:  0.001458511222153902\n",
      "epoch/batch:  41 41  loss:  0.0033511307556182146\n",
      "epoch/batch:  42 42  loss:  0.0025554050225764513\n",
      "epoch/batch:  43 43  loss:  0.0013355781557038426\n",
      "epoch/batch:  44 44  loss:  0.0050332434475421906\n",
      "epoch/batch:  45 45  loss:  0.0026193910744041204\n",
      "epoch/batch:  46 46  loss:  0.0016568265855312347\n",
      "epoch/batch:  47 47  loss:  0.0023766723461449146\n",
      "epoch/batch:  48 48  loss:  0.003993798512965441\n",
      "epoch/batch:  49 49  loss:  0.0034679488744586706\n",
      "epoch/batch:  50 50  loss:  0.003127817064523697\n",
      "epoch/batch:  51 51  loss:  0.002401430858299136\n",
      "epoch/batch:  52 52  loss:  0.0036702880170196295\n",
      "epoch/batch:  53 53  loss:  0.0036292995791882277\n",
      "epoch/batch:  54 54  loss:  0.0037764650769531727\n",
      "epoch/batch:  55 55  loss:  0.005252803675830364\n",
      "epoch/batch:  56 56  loss:  0.002698217285797\n",
      "epoch/batch:  57 57  loss:  0.0011086382437497377\n",
      "epoch/batch:  58 58  loss:  0.00285642733797431\n",
      "epoch/batch:  59 59  loss:  0.00327101768925786\n",
      "epoch/batch:  60 60  loss:  0.0045943609438836575\n",
      "epoch/batch:  61 61  loss:  0.004668483044952154\n",
      "epoch/batch:  62 62  loss:  0.002844323171302676\n",
      "epoch/batch:  63 63  loss:  0.0018004454905167222\n",
      "epoch/batch:  64 64  loss:  0.0017349951667711139\n",
      "epoch/batch:  65 65  loss:  0.002632583724334836\n",
      "epoch/batch:  66 66  loss:  0.001149606192484498\n",
      "epoch/batch:  67 67  loss:  0.0036504908930510283\n",
      "epoch/batch:  68 68  loss:  0.003583088517189026\n",
      "epoch/batch:  69 69  loss:  0.0011687048245221376\n",
      "epoch/batch:  70 70  loss:  0.0014625912299379706\n",
      "epoch/batch:  71 71  loss:  0.004874053876847029\n",
      "epoch/batch:  72 72  loss:  0.0026684000622481108\n",
      "epoch/batch:  73 73  loss:  0.0014317597961053252\n",
      "epoch/batch:  74 74  loss:  0.0034266640432178974\n",
      "epoch/batch:  75 75  loss:  0.0021056439727544785\n",
      "epoch/batch:  76 76  loss:  0.0019211905309930444\n",
      "epoch/batch:  77 77  loss:  0.0026365581434220076\n",
      "epoch/batch:  78 78  loss:  0.006405688356608152\n",
      "epoch/batch:  79 79  loss:  0.004894623067229986\n",
      "epoch/batch:  80 80  loss:  0.001568849547766149\n",
      "epoch/batch:  81 81  loss:  0.004331186879426241\n",
      "tensor([[0.9861],\n",
      "        [2.9893],\n",
      "        [1.9893],\n",
      "        [3.9865],\n",
      "        [4.9945]], device='cuda:0', requires_grad=True) tensor([1.2596], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# =============\n",
    "# train a model to find hidden \"weights and bias\" from training data.\n",
    "# TODO, practice with learning rate equal to 0.005, 0.02 0.01 to see the result how the model work \n",
    "# =============\n",
    "\n",
    "# create a new model\n",
    "model = ModelLinear(5,1)\n",
    "\n",
    "# train\n",
    "lr = 0.002\n",
    "\n",
    "i=0\n",
    "MAX_EPOCHS=5000\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    loss = 100000\n",
    "    for batch in train_loader:\n",
    "\n",
    "        x, y_gold = batch\n",
    "  \n",
    "        x = x.cuda()\n",
    "        y_gold = y_gold.cuda()\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x)\n",
    "\n",
    "\n",
    "        # Compute and loss = average ((out_put - pred) ^ 2)\n",
    "        total_sample_in_mini_batch = y_pred.shape[0] \n",
    "        loss = sum((y_pred - y_gold) * (y_pred - y_gold)) * 1/total_sample_in_mini_batch\n",
    "\n",
    "        # perform a backward pass (backpropagation) => to get the gradient values in Tensor weights\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step(), this similar to the SGD algorithm \n",
    "        with torch.no_grad():\n",
    "            for param in model.get_parameters():\n",
    "                param.add_(-lr * param.grad)\n",
    "                param.grad.fill_(0)\n",
    "    \n",
    "    if loss.item() < 0.001:\n",
    "        break\n",
    "    print('epoch/batch: ', epoch, i,' loss: ', loss.item())\n",
    "    i+=1\n",
    "\n",
    "print(model.weight,  model.bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3602b4515afb2d87a870b61c65d7b658117eca8f37f64d20593019ba04f7019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
