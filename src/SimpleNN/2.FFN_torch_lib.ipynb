{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0., 1., 5., 4.],\n",
      "        [0., 5., 3., 7., 6.],\n",
      "        [5., 3., 4., 6., 8.]])\n",
      "tensor([[45.],\n",
      "        [80.],\n",
      "        [87.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2847809/2941274300.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  x_train =  torch.Tensor([rand_5_numbers_from1to10() for i in range(60)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    " \n",
    "# =====================\n",
    "# generate data\n",
    "def gen_fake_data(hidden_weight, hidden_bias):\n",
    "    def rand_5_numbers_from1to10():\n",
    "        return np.random.permutation(10)[:5]\n",
    "\n",
    "    x_train =  torch.Tensor([rand_5_numbers_from1to10() for i in range(60)]) \n",
    "    x_test = torch.Tensor([rand_5_numbers_from1to10() for i in range(60)]) \n",
    "    \n",
    "    hidden_weight = torch.Tensor(hidden_weight).unsqueeze(-1) \n",
    "    hidden_bias = torch.Tensor([hidden_bias]) \n",
    "\n",
    "    y_train = torch.mm(x_train, hidden_weight) + hidden_bias\n",
    "    y_test = torch.mm(x_test, hidden_weight) + hidden_bias\n",
    "    return x_train, y_train, x_test, y_test\n",
    "     \n",
    "# gen training data sample \n",
    "x_train, y_train, x_test, y_test = gen_fake_data(hidden_weight=[1,3,2,4,5], hidden_bias=1)\n",
    "print(x_train[:3])\n",
    "print(y_train[:3])\n",
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataloader\n",
    "# dataset_example should support operator index_selection for create the data_loader object\n",
    "train_loader = DataLoader([(x_train[i], y_train[i]) for i in range(x_train.shape[0])], batch_size=10, collate_fn=None, shuffle=True)\n",
    "test_loader = DataLoader([(x_test[i], y_test[i]) for i in range(x_test.shape[0])], batch_size=50, collate_fn=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice part \n",
    "- train a model to find hidden \"weights and bias\" from training data.\n",
    "- **TODO**, practice with learning rate equal to 0.005, 0.02 0.01 to see the result how the model work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/batch:  0  loss:  45.8241081237793\n",
      "epoch/batch:  1  loss:  5.250632286071777\n",
      "epoch/batch:  2  loss:  2.9515390396118164\n",
      "epoch/batch:  3  loss:  0.9043130874633789\n",
      "epoch/batch:  4  loss:  0.17148271203041077\n",
      "epoch/batch:  5  loss:  0.06080832704901695\n",
      "epoch/batch:  6  loss:  0.02346581034362316\n",
      "epoch/batch:  7  loss:  0.021725833415985107\n",
      "epoch/batch:  8  loss:  0.013775328174233437\n",
      "epoch/batch:  9  loss:  0.017068536952137947\n",
      "epoch/batch:  10  loss:  0.013331432826817036\n",
      "epoch/batch:  11  loss:  0.006817650981247425\n",
      "epoch/batch:  12  loss:  0.00743310758844018\n",
      "epoch/batch:  13  loss:  0.011507446877658367\n",
      "epoch/batch:  14  loss:  0.005524070467799902\n",
      "epoch/batch:  15  loss:  0.0071863518096506596\n",
      "epoch/batch:  16  loss:  0.004175671376287937\n",
      "epoch/batch:  17  loss:  0.006212280131876469\n",
      "epoch/batch:  18  loss:  0.004982548300176859\n",
      "epoch/batch:  19  loss:  0.00919510331004858\n",
      "epoch/batch:  20  loss:  0.004378370475023985\n",
      "epoch/batch:  21  loss:  0.007110916078090668\n",
      "epoch/batch:  22  loss:  0.004428270272910595\n",
      "epoch/batch:  23  loss:  0.009897584095597267\n",
      "epoch/batch:  24  loss:  0.008467445150017738\n",
      "epoch/batch:  25  loss:  0.007262102793902159\n",
      "epoch/batch:  26  loss:  0.003508737776428461\n",
      "epoch/batch:  27  loss:  0.007909952662885189\n",
      "epoch/batch:  28  loss:  0.004710344597697258\n",
      "epoch/batch:  29  loss:  0.012384180910885334\n",
      "epoch/batch:  30  loss:  0.006354306824505329\n",
      "epoch/batch:  31  loss:  0.007440043147653341\n",
      "epoch/batch:  32  loss:  0.0028856450226157904\n",
      "epoch/batch:  33  loss:  0.007184991147369146\n",
      "epoch/batch:  34  loss:  0.002531249774619937\n",
      "epoch/batch:  35  loss:  0.005406026728451252\n",
      "epoch/batch:  36  loss:  0.00738265085965395\n",
      "epoch/batch:  37  loss:  0.007530701346695423\n",
      "epoch/batch:  38  loss:  0.0022976300679147243\n",
      "epoch/batch:  39  loss:  0.0026073891203850508\n",
      "epoch/batch:  40  loss:  0.0019505011150613427\n",
      "epoch/batch:  41  loss:  0.003927100449800491\n",
      "epoch/batch:  42  loss:  0.006718438118696213\n",
      "epoch/batch:  43  loss:  0.00820690207183361\n",
      "epoch/batch:  44  loss:  0.006163062062114477\n",
      "epoch/batch:  45  loss:  0.002184225246310234\n",
      "epoch/batch:  46  loss:  0.002091340022161603\n",
      "epoch/batch:  47  loss:  0.00464779743924737\n",
      "epoch/batch:  48  loss:  0.0019933104049414396\n",
      "epoch/batch:  49  loss:  0.007942385971546173\n",
      "epoch/batch:  50  loss:  0.005003713071346283\n",
      "epoch/batch:  51  loss:  0.00596793694421649\n",
      "epoch/batch:  52  loss:  0.007017293479293585\n",
      "epoch/batch:  53  loss:  0.007395479362457991\n",
      "epoch/batch:  54  loss:  0.006853894796222448\n",
      "epoch/batch:  55  loss:  0.008479547686874866\n",
      "epoch/batch:  56  loss:  0.00786285288631916\n",
      "epoch/batch:  57  loss:  0.010242261923849583\n",
      "epoch/batch:  58  loss:  0.006299220956861973\n",
      "epoch/batch:  59  loss:  0.008755010552704334\n",
      "epoch/batch:  60  loss:  0.004257763270288706\n",
      "epoch/batch:  61  loss:  0.006109051406383514\n",
      "epoch/batch:  62  loss:  0.0061837914399802685\n",
      "epoch/batch:  63  loss:  0.0045308396220207214\n",
      "epoch/batch:  64  loss:  0.003996638115495443\n",
      "epoch/batch:  65  loss:  0.005110441241413355\n",
      "epoch/batch:  66  loss:  0.007645857520401478\n",
      "epoch/batch:  67  loss:  0.004360267426818609\n",
      "epoch/batch:  68  loss:  0.0068771871738135815\n",
      "epoch/batch:  69  loss:  0.004538797773420811\n",
      "epoch/batch:  70  loss:  0.0017657962162047625\n",
      "epoch/batch:  71  loss:  0.00288918474689126\n",
      "epoch/batch:  72  loss:  0.005890544503927231\n",
      "epoch/batch:  73  loss:  0.006460874807089567\n",
      "epoch/batch:  74  loss:  0.004321813117712736\n",
      "epoch/batch:  75  loss:  0.005989572498947382\n",
      "epoch/batch:  76  loss:  0.007408958859741688\n",
      "epoch/batch:  77  loss:  0.0044936928898096085\n",
      "epoch/batch:  78  loss:  0.004555385559797287\n",
      "epoch/batch:  79  loss:  0.0043212734162807465\n",
      "epoch/batch:  80  loss:  0.0028831306844949722\n",
      "epoch/batch:  81  loss:  0.004360425751656294\n",
      "epoch/batch:  82  loss:  0.005365666002035141\n",
      "epoch/batch:  83  loss:  0.002916069468483329\n",
      "epoch/batch:  84  loss:  0.005067750811576843\n",
      "epoch/batch:  85  loss:  0.002873797668144107\n",
      "epoch/batch:  86  loss:  0.005908140446990728\n",
      "epoch/batch:  87  loss:  0.0058844140730798244\n",
      "epoch/batch:  88  loss:  0.002119284588843584\n",
      "epoch/batch:  89  loss:  0.00572088873013854\n",
      "epoch/batch:  90  loss:  0.009041567333042622\n",
      "epoch/batch:  91  loss:  0.003100642701610923\n",
      "epoch/batch:  92  loss:  0.0030369076412171125\n",
      "epoch/batch:  93  loss:  0.003457678249105811\n",
      "epoch/batch:  94  loss:  0.004402518272399902\n",
      "epoch/batch:  95  loss:  0.0041107009164988995\n",
      "epoch/batch:  96  loss:  0.007616633083671331\n",
      "epoch/batch:  97  loss:  0.007665467448532581\n",
      "epoch/batch:  98  loss:  0.00221346877515316\n",
      "epoch/batch:  99  loss:  0.003672447055578232\n",
      "epoch/batch:  100  loss:  0.006650600582361221\n",
      "epoch/batch:  101  loss:  0.0026105407159775496\n",
      "epoch/batch:  102  loss:  0.006217000540345907\n",
      "epoch/batch:  103  loss:  0.0030641446355730295\n",
      "epoch/batch:  104  loss:  0.004975458141416311\n",
      "epoch/batch:  105  loss:  0.0035327677614986897\n",
      "epoch/batch:  106  loss:  0.0021165211219340563\n",
      "epoch/batch:  107  loss:  0.003912921529263258\n",
      "epoch/batch:  108  loss:  0.002864954760298133\n",
      "epoch/batch:  109  loss:  0.004878672305494547\n",
      "epoch/batch:  110  loss:  0.003838901175186038\n",
      "epoch/batch:  111  loss:  0.00475105969235301\n",
      "epoch/batch:  112  loss:  0.0052824183367192745\n",
      "epoch/batch:  113  loss:  0.007331902626901865\n",
      "epoch/batch:  114  loss:  0.005165429320186377\n",
      "epoch/batch:  115  loss:  0.00407362962141633\n",
      "epoch/batch:  116  loss:  0.002741203410550952\n",
      "epoch/batch:  117  loss:  0.0049781217239797115\n",
      "epoch/batch:  118  loss:  0.00394069030880928\n",
      "epoch/batch:  119  loss:  0.00519096152856946\n",
      "epoch/batch:  120  loss:  0.0047507272101938725\n",
      "epoch/batch:  121  loss:  0.007348756771534681\n",
      "epoch/batch:  122  loss:  0.0031037963926792145\n",
      "epoch/batch:  123  loss:  0.002649378962814808\n",
      "epoch/batch:  124  loss:  0.0026877231430262327\n",
      "epoch/batch:  125  loss:  0.003557924646884203\n",
      "epoch/batch:  126  loss:  0.006450358312577009\n",
      "epoch/batch:  127  loss:  0.0032726468052715063\n",
      "epoch/batch:  128  loss:  0.0031191909220069647\n",
      "epoch/batch:  129  loss:  0.0019650585018098354\n",
      "epoch/batch:  130  loss:  0.002829604549333453\n",
      "epoch/batch:  131  loss:  0.0023540975525975227\n",
      "epoch/batch:  132  loss:  0.0038674373645335436\n",
      "epoch/batch:  133  loss:  0.002301798202097416\n",
      "epoch/batch:  134  loss:  0.004767300561070442\n",
      "epoch/batch:  135  loss:  0.004375522490590811\n",
      "epoch/batch:  136  loss:  0.0025477451272308826\n",
      "epoch/batch:  137  loss:  0.0038758949376642704\n",
      "epoch/batch:  138  loss:  0.0031033975537866354\n",
      "epoch/batch:  139  loss:  0.001274602022022009\n",
      "epoch/batch:  140  loss:  0.0027917674742639065\n",
      "epoch/batch:  141  loss:  0.003342946758493781\n",
      "epoch/batch:  142  loss:  0.0047256723046302795\n",
      "epoch/batch:  143  loss:  0.0012877522967755795\n",
      "epoch/batch:  144  loss:  0.0035542678087949753\n",
      "epoch/batch:  145  loss:  0.001992190722376108\n",
      "epoch/batch:  146  loss:  0.001544714323244989\n",
      "epoch/batch:  147  loss:  0.0023334703873842955\n",
      "epoch/batch:  148  loss:  0.003074310254305601\n",
      "epoch/batch:  149  loss:  0.0031342082656919956\n",
      "epoch/batch:  150  loss:  0.0032051950693130493\n",
      "epoch/batch:  151  loss:  0.005402380134910345\n",
      "epoch/batch:  152  loss:  0.004700254183262587\n",
      "epoch/batch:  153  loss:  0.0033233545254915953\n",
      "epoch/batch:  154  loss:  0.0023521126713603735\n",
      "epoch/batch:  155  loss:  0.003507044166326523\n",
      "epoch/batch:  156  loss:  0.0029743448831140995\n",
      "epoch/batch:  157  loss:  0.0033467242028564215\n",
      "epoch/batch:  158  loss:  0.0025605717673897743\n",
      "epoch/batch:  159  loss:  0.0031474437564611435\n",
      "epoch/batch:  160  loss:  0.003486874047666788\n",
      "epoch/batch:  161  loss:  0.0023610906209796667\n",
      "epoch/batch:  162  loss:  0.0014463807456195354\n",
      "epoch/batch:  163  loss:  0.004441733006387949\n",
      "epoch/batch:  164  loss:  0.0026938156224787235\n",
      "epoch/batch:  165  loss:  0.003883105469867587\n",
      "epoch/batch:  166  loss:  0.0015430761268362403\n",
      "epoch/batch:  167  loss:  0.0031918571330606937\n",
      "epoch/batch:  168  loss:  0.0011607682099565864\n",
      "epoch/batch:  169  loss:  0.0030421081464737654\n",
      "epoch/batch:  170  loss:  0.00420650839805603\n",
      "epoch/batch:  171  loss:  0.0029077541548758745\n",
      "epoch/batch:  172  loss:  0.00437949551269412\n",
      "epoch/batch:  173  loss:  0.002277717227116227\n",
      "epoch/batch:  174  loss:  0.002989252796396613\n",
      "epoch/batch:  175  loss:  0.00151354749687016\n",
      "epoch/batch:  176  loss:  0.007118618581444025\n",
      "epoch/batch:  177  loss:  0.002396223833784461\n",
      "epoch/batch:  178  loss:  0.0017519596731290221\n",
      "epoch/batch:  179  loss:  0.002572468016296625\n",
      "epoch/batch:  180  loss:  0.0014600217109546065\n",
      "epoch/batch:  181  loss:  0.0019208000740036368\n",
      "epoch/batch:  182  loss:  0.002832606201991439\n",
      "epoch/batch:  183  loss:  0.002760697156190872\n",
      "epoch/batch:  184  loss:  0.005595421884208918\n",
      "epoch/batch:  185  loss:  0.0033067993354052305\n",
      "epoch/batch:  186  loss:  0.0022809510119259357\n",
      "epoch/batch:  187  loss:  0.002183981705456972\n",
      "epoch/batch:  188  loss:  0.0018749971641227603\n",
      "epoch/batch:  189  loss:  0.0028897852171212435\n",
      "epoch/batch:  190  loss:  0.0036548369098454714\n",
      "epoch/batch:  191  loss:  0.0029960121028125286\n",
      "epoch/batch:  192  loss:  0.003778960555791855\n",
      "epoch/batch:  193  loss:  0.0019498271867632866\n",
      "epoch/batch:  194  loss:  0.0034212300088256598\n",
      "epoch/batch:  195  loss:  0.00291761034168303\n",
      "epoch/batch:  196  loss:  0.003606257727369666\n",
      "epoch/batch:  197  loss:  0.002522407565265894\n",
      "epoch/batch:  198  loss:  0.004183808341622353\n",
      "epoch/batch:  199  loss:  0.002504941774532199\n",
      "epoch/batch:  200  loss:  0.002200242131948471\n",
      "epoch/batch:  201  loss:  0.002275983802974224\n",
      "epoch/batch:  202  loss:  0.0034275136422365904\n",
      "epoch/batch:  203  loss:  0.0032302227336913347\n",
      "epoch/batch:  204  loss:  0.003439313266426325\n",
      "Parameter containing:\n",
      "tensor([[1.0114, 3.0077, 2.0088, 4.0085, 5.0140]], device='cuda:0',\n",
      "       requires_grad=True) Parameter containing:\n",
      "tensor([0.7605], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# train a model to find hidden \"weights and bias\" from training data.\n",
    "# TODO, practice with learning rate equal to 0.005, 0.02 0.01 to see the result how the model work \n",
    "\n",
    "# ============= ############## ===============\n",
    "# USE LIBRARY\n",
    "model = torch.nn.Linear(5,1)\n",
    "model.cuda()\n",
    "\n",
    "# loss also is supported by a library \n",
    "loss_computation = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.005)\n",
    "# ============= ############## ===============\n",
    "\n",
    "MAX_EPOCHS=5000\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    loss = 100000\n",
    "    for batch in train_loader:\n",
    "\n",
    "        x, y_gold = batch\n",
    "  \n",
    "        x = x.cuda()\n",
    "        y_gold = y_gold.cuda()\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x)\n",
    "\n",
    "\n",
    "        # ============= ############## ===============\n",
    "        # Compute and loss = average ((out_put - pred) ^ 2)\n",
    "        loss = loss_computation(y_pred, y_gold)\n",
    "        # ============= ############## ===============\n",
    "\n",
    "        # perform a backward pass (backpropagation) => to compute the gradient values in Tensor weights\n",
    "        loss.backward()\n",
    "\n",
    "        # ============= ############## ===============\n",
    "        # USE LIBRARY: 'model.parameters()' in stead of 'model.get_parameter()' is implemented by library, also return list of parameters: \"weight\" and \"bias\" \n",
    "        # Optimizer step(), this update gradient values to weights.\n",
    "        optimizer.step() # instead of `param.add_(-lr * param.grad)` => update weight values\n",
    "        optimizer.zero_grad() # instead of `param.grad.fill_(0)` => remove all the old gradient values in all Tensor weight\n",
    "        # ============= ############## ===============\n",
    "    \n",
    "    if loss.item() < 0.001:\n",
    "        break\n",
    "    print('epoch/batch: ', epoch, ' loss: ', loss.item())\n",
    "\n",
    "print(model.weight,  model.bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3602b4515afb2d87a870b61c65d7b658117eca8f37f64d20593019ba04f7019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
