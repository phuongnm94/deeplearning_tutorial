## Hyper-parameters (hp) fine-tunning 

**Idea:** One model usully contains many hyper-parameters, when we find the impact of one hyper parameter (e.g., learning rate) to the model, we have to froze other hyper parameters as a same context. 

--- 
For example: hp set = (A, B, C) with A = {a1, a2, a3}, B = {b1, b2, b3}, C = {c1, c2,c3}. For all the candidates (greedy search), it should be total: len(A)\*len(B)\*len(C) = 3\*3\*3 = 27 checking cases. However, we have no time for greedy search, we follow the strategy bellow:

To find the best value of hp A, we check  settings {A=**a1**, B=b1, C=c1}, {A=**a2**, B=b1, C=c1}, {A=**a3**, B=b1, C=c1}. => assume that we found **A = a3** is the best

Then, with the previous best result (A = a3), continue to find the best value of hp B, with {A=a3, **B=b2**, C=c1}, {A=a3, **B=b3**, C=c1}. No need to try {A=a3, **B=b1**, C=c1} because this result was in the previous checking.  => assume that we found **B = b1** is the best.

Then, with the previous best result, continue find the best value of hp B, with {A=a3, B=b1, C=c2}, {A=a3, B=b1, C=c3},  => found **B = b1** is the best

---

- try the code [9.iemocap_roberta_answer.py](9.iemocap_roberta_answer.py)  with follow hp sets with data `iemocap.testwindow2.flatten.json`
  - check the best dropout in {0.1, **0.2**, 0.3}
  - check the batch size in {4, **8**, 16}
  - check the best learning rate in {5e-6, **1e-5**, 2e-5, 1e-4}. 
  
  > Note: the bold values is the values we found the best setting current `dev_f1 = 61.44`, `test_f1=62.32`. You can check this result again.