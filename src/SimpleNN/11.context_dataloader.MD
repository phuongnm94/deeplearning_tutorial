- given this data input containing many conversations like this:
    ```json  
    [
        # this is first conversation 
        { 
            "labels": [
            4,
            2,
            4,
            4 
            ],
            "sentences": [
            "Guess what?",
            "what?",
            "I did it, I asked her to marry me.",
            "Yes, I did it."
            ],
            "sentences_mixed_around": [
                ...
            ],
            "s_id": "Ses05M_impro03",
            "genders": [
            "M",
            "F",
            "M",
            "M",
            "F", 
            ]
        },

        # this is second conversation 
        { 
            "labels": [
            4,
            2,
            ],
            "sentences": [
            "Guess what?",
            "what?", 
            ],
            "sentences_mixed_around": [
                ...
            ],
            "s_id": "Ses05M_impro03",
            "genders": [
            "M",
            "F",  
            ]
        }
    ]

    ```
- requirements:
  - we need to create a dataloader considering each conversation as a input. 
    - for example, if batch size = 2, we created a batch containing all utterances of 2 conversations. 
  - model need to encode all the utterances of 2 conversations in the same time. 
  - each conversation have different number of utterances (#conversation), therefore, to batching, we need to check which conversation have largest number of utterance (#conversation), and padding to max length of each conversation. 
    - for example, the above sample have max #conversation = 4, if we combine first and second conversations
  - we need to create a conversation masked to point it out which utterance is masked in the tensor. 
  - in the code:
    - sentences_ids = sentence ids of all uttereances in all conversations `shape = (number_of_conversation x longest_conversation x longest_sentence)`
    - torch.LongTensor(labels) = label of all uttereances in all conversations
    - padding_utterance_masked: `shape = (number_of_conversation x longest_conversation)`. in this example should be (2x4) with batch size =2 and longest conversation =4. => [[False, False False False], [False False True True]]. The padding utterance position is masked True.
    - tips:
      - **sentence padding process**: the padding words (\<PAD\>) is added to all the sentence to achieve the longest sent, which supported by BERT/RoBERTa Tokenizer:
        - for examples, sentence padding process is supported by `bert_tokenizer`:
          - origin: ["this is a", "this is a sentence", "this is"] -> longest sentence contain 4 words
          - padded: ["this is a \<pad>", "this is a sentence", "this is \<pad> \<pad>"] 
      - **conversasion padding process**: the padding sentence ("\<pad_sentence>" ) is added to all the conversation to achieve the longest conversation, (we need to do by manually):
          - origin: [["this is a"], [ "this is a", "this is a sentence", "this is"]] -> longest conversation contain 3 sentences
          - padded: [["this is a", "\<pad_sentence>" , "\<pad_sentence>"  ] , ["this is a", "this is a sentence", "this is"]] 
      - **sentence label padding process**: each sentence should have one label, we usually use -1 for label padding:
          - origin: [["this is a"], [ "this is a", "this is a sentence", "this is"]] 
            - corresponding labels: [[1], [ 1, 2, 4]]
          - padded conversation: [["this is a", "\<pad_sentence>" , "\<pad_sentence>"  ] , ["this is a", "this is a sentence", "this is"]] 
            - padded label: [[1, -1 , -1 ] , [ 1, 2, 4]] 