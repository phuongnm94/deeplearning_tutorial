{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "![emo](../../img/lstm_history.jpg)\n",
    "\n",
    "Use LSTM learn the intra speaker utterances - sequence features\n",
    "- select all the utterances of each user in a conversation \n",
    "- stack all vectors of all user and forward via LSTM \n",
    "- add back the vectors fused by LSTM architecture to list of utterances. => `utterance_vector_fused_by_speaker_history`\n",
    "- Use `utterance_vector_fused_by_speaker_history` as a new features concat with `y_hat`  before Linear to the output layer. \n",
    "\n",
    "The `speaker_history_model_by_lstm` modeling learn relations of all utterances of one speaker sequentialy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 7])\n",
      "shape of fist and second speaker vectors:  3 4\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "d_model = 1024\n",
    "embedding_size = d_model\n",
    "batch_size = 1\n",
    "sequence_length = 7\n",
    "\n",
    "\n",
    "# ================ \n",
    "# fake input data \n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"set seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_random_seed(2)\n",
    "\n",
    "fake_utterance_vector_from_bert = torch.rand(batch_size, sequence_length, embedding_size)\n",
    "intra_speaker_masekd_all = torch.BoolTensor(batch_size, sequence_length, sequence_length)\n",
    "labels = torch.LongTensor(torch.randint(0,5, size=[batch_size, sequence_length]))\n",
    "print(intra_speaker_masekd_all.shape)\n",
    "for k in range(batch_size):\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(sequence_length):\n",
    "            v = random.choice([True, False]) \n",
    "            intra_speaker_masekd_all[k,i,j] = v\n",
    "            intra_speaker_masekd_all[k,j,i] = v\n",
    "fake_mask=torch.LongTensor(batch_size, sequence_length).fill_(0)\n",
    "fake_mask[:, -2] = 1\n",
    "fake_mask[:, -1] = 1\n",
    "fake_mask = (fake_mask == 1)\n",
    "fake_embedding = torch.rand(batch_size, sequence_length, embedding_size)\n",
    "\n",
    "# ================ \n",
    "# init model \n",
    "speaker_history_model_by_lstm = nn.LSTM(input_size=d_model, hidden_size=d_model//2, num_layers=2, \n",
    "                     dropout=0.2, batch_first=True, bidirectional=True)\n",
    " \n",
    "# ================ \n",
    "# forward \n",
    "first_user_mask = intra_speaker_masekd_all[:,0]\n",
    "second_user_mask = ~intra_speaker_masekd_all[:, 0]\n",
    "\n",
    "# separate utterance each speakers \n",
    "utterance_vector_fused_by_speaker_history = fake_utterance_vector_from_bert + 0 # for create a new tensor equal to output bert vector`fake_utterance_vector_from_bert`\n",
    "v_first_speaker = utterance_vector_fused_by_speaker_history[first_user_mask] \n",
    "v_second_speaker = utterance_vector_fused_by_speaker_history[second_user_mask] \n",
    "\n",
    "# padding \n",
    "n_utterance_speaker = v_first_speaker.shape[0], v_second_speaker.shape[0]\n",
    "max_n_utterance = max(n_utterance_speaker)\n",
    "if v_first_speaker.shape[0] < v_second_speaker.shape[0]:\n",
    "    v_first_speaker = F.pad(v_first_speaker, [0, 0, 0, max_n_utterance-v_first_speaker.shape[0]])\n",
    "else:\n",
    "    v_second_speaker = F.pad(v_second_speaker, [0, 0, 0,  max_n_utterance-v_second_speaker.shape[0]])\n",
    "v_all_speakers = torch.stack([v_first_speaker, v_second_speaker], dim=0)\n",
    "\n",
    "# learn history context each user utterances \n",
    "h_words, (hn, cn) = speaker_history_model_by_lstm(v_all_speakers) \n",
    "\n",
    "# put the lstm output back to the final hidden features\n",
    "# fake_utterance_vector_from_bert is fused by lstm history features \n",
    "utterance_vector_fused_by_speaker_history[first_user_mask] += h_words[0][:n_utterance_speaker[0]]\n",
    "utterance_vector_fused_by_speaker_history[second_user_mask] += h_words[1][:n_utterance_speaker[1]]\n",
    "utterance_vector_fused_by_speaker_history = utterance_vector_fused_by_speaker_history.reshape(batch_size*sequence_length, -1)\n",
    "\n",
    "print(\"shape of fist and second speaker vectors: \", n_utterance_speaker[0], n_utterance_speaker[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3602b4515afb2d87a870b61c65d7b658117eca8f37f64d20593019ba04f7019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
