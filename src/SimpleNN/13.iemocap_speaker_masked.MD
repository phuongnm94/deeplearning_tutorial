- given this data input containing many conversations like this:  
  ![img](../../img/masked.png)
--- 

### Masked tensor 
- requirements: construct the masked tensor to save the utterances speaker information. 
- output requirements: 
    - **intra_speaker_masekd**: a 2 dimmentions tensor (or list of list) A, wherer `Aij == True` if `speaker(u_i) == speaker(u_j)` => shape = (n_conversation, len_longest_conversation, len_longest_conversation)
    - **inter_speaker_masked**: a 2 dimmentions tensor (or list of list) A, wherer `Aij == True` if `speaker(u_i) != speaker(u_j)`  => shape = (n_conversation, len_longest_conversation, len_longest_conversation)
- tips:  
  - create a 2 dimmensions list (list in list by using 2 for loop). Remember that the information of speaker is saved in the `sample['genders']` where sample is a conversation which is one element of batch.  The same genders is the same speaker. 
  - `inter_speaker_masked` is unnecessary because just use simple way to create: `inter_speaker_masked = ~intra_speaker_masekd` to revert flag `true` to `false`
--- 
### Inter-speaker and Intra-speaker modeling 
- requirements: construct utterance vector information by incoporating inter-speaker or intra-speaker 
- examples: 
  ```python 
  import torch 
  from torch import nn

  # ===========
  # fake data randomly 
  # this part is not important 
  batch_size = 1
  seq_len = 48
  hidden_size = 1024
  num_labels = 6
  vector_utterances_without_context = torch.rand(batch_size, seq_len, hidden_size)
  y_hat = torch.rand(batch_size, seq_len, num_labels)
  intra_speaker_masked = (torch.rand(batch_size, seq_len, seq_len) > -0.5)
  intra_speaker_masked[:, :seq_len-5, :seq_len-5] = False
  padding_utterance_masked =  torch.rand(batch_size, seq_len) > -0.5
  padding_utterance_masked[:, :seq_len-5] = False
  # ===========

  # model init 
  d_model = hidden_size
  inter_speaker_modeling = nn.MultiheadAttention(int(d_model/2), num_heads=8, dropout=0.2, batch_first=True)
  inter_q =   nn.Linear(d_model, int(d_model/2))  
  inter_k =   nn.Linear(d_model,int(d_model/2))  
  inter_v =   nn.Linear(d_model,int(d_model/2))  
  inter_speaker_2_output_layer =   nn.Linear(int(d_model/2), num_labels)  

  # forward model 
  q_vector = inter_q(vector_utterances_without_context)
  k_vector = inter_k(vector_utterances_without_context)
  v_vector = inter_v(vector_utterances_without_context)
  vector_fused_by_inter_speaker, attentions = inter_speaker_modeling(q_vector, k_vector, v_vector, attn_mask=(intra_speaker_masked).repeat(8,1,1), key_padding_mask=padding_utterance_masked)
  output_vector = inter_speaker_2_output_layer(vector_fused_by_inter_speaker)
  output_vector = output_vector.reshape(batch_size*seq_len, -1)

  # output shape 
  # incoportate `output_vector` to the `y_hat`
  y_hat = y_hat +  output_vector

  print(output_vector)
  print(output_vector.shape)
  ```