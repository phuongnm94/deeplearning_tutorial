- given this data input containing many conversations like this:
    ```json  
    [
        # this is first conversation 
        { 
            "labels": [
            4,
            2,
            4,
            4 
            ],
            "sentences": [
            "Guess what?",
            "what?",
            "I did it, I asked her to marry me.",
            "Yes, I did it."
            ],
            "sentences_mixed_around": [
                ...
            ],
            "s_id": "Ses05M_impro03",
            "genders": [
            "M",
            "F",
            "M",
            "M",
            "F", 
            ]
        },

        # this is second conversation 
        { 
            "labels": [
            4,
            2,
            ],
            "sentences": [
            "Guess what?",
            "what?", 
            ],
            "sentences_mixed_around": [
                ...
            ],
            "s_id": "Ses05M_impro03",
            "genders": [
            "M",
            "F",  
            ]
        }
    ]

    ```
- requirements: use Transformer model to learn the dependencies between utterances in a conversation. 
- output requirements: 
    - **sentence_vectors**: bert encoding for all utterances in all conversations  => shape = (n_conversation*len_longest_conversation, hiden_size), where n_conversation is batch_size
    - **u_vector_fused_by_context**: utterance vectors fused by context using self-attention mechanism (Transformer) => shape = (n_conversation*len_longest_conversation, hiden_size)
- tips (check this Transformer again [6.Transformer_engine.ipynb](./6.Transformer_engine.ipynb) ):
    1. use pretrained model (Roberta) to encode all the utterances of all conversations. Use first token ([CLS]) to get representation of all utterances/sentences:
    
        ```python
        utterance_vectors = roberta_model(**output_from_bert_tokenizer)[1]
        ```
    2. use Transfomrer layer to learn dependencies between utterances
    
        ```python
        # reshape the utterances => n_conversation x len_longest_conversation x hidden size
        sentence_vectors_with_convers_shape = sentence_vectors.reshape(n_conversation, len_longest_conversation, -1)

        # context modeling need to use utterances with conversation shape
        u_vector_fused_by_context = transformer_model (sentence_vectors_with_convers_shape + position_encoding(sentence_vectors_with_convers_shape), src_key_padding_mask=padding_utterance_masked)

        # reshape again for flatten all the utterances of all conversations
        u_vector_fused_by_context = u_vector_fused_by_context.reshape(n_conversation*len_longest_conversation, -1)
        ```