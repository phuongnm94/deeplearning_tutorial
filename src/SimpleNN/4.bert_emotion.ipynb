{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model insight and analysis\n",
    "\n",
    "**Previous model:** We assum that, each word in the input sentence have a weight affect to the label. To this end, we need to learn these hidden weights. *We designed a Embedding layer (random init) to convert word to vector. Then `sum`  embedding vectors of all words in a input sentence, then transform document vector to the label values (images)*\n",
    "\n",
    "![model_arc](../../img/dl_tutorial-Trang-2.drawio.png)\n",
    "\n",
    "**Questions?** \n",
    "- If the order of words in the sentence change, the performance of this model will change or not? Why?\n",
    "- what difference thing model learn in embedding layer and the Linear output layer ?  \n",
    "- what did model learn? or which words strongly affected to the label ? For example, given sentence \"im updating my because i feel shitty\" => label is \"sadness\" , how we know that model learned the correct thing that shitty is the sad emotion? \n",
    "- Can we manually create a embedding vector for a new word (e.g. \"wa_ta_shi\") to achieve current performance without train model again? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/phuongnm/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520cb7b97a0e412b80c477e70bbb5f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    " \n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, AutoTokenizer, AutoModel\n",
    "\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import random\n",
    "# =====================\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"set seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_random_seed(7)\n",
    "\n",
    "dataset = load_dataset('emotion')\n",
    "train_data = dataset['train']\n",
    "all_labels = []\n",
    "for sample in train_data:\n",
    "    all_labels.append(sample['label'])\n",
    "\n",
    "\n",
    "# count label \n",
    "num_labels = len(set(all_labels))\n",
    " \n",
    "\n",
    "# init model \n",
    "# Load config from pretrained name or path \n",
    "pre_trained_model_name = 'roberta-base'\n",
    "config = BertConfig.from_pretrained(pre_trained_model_name)  # Load pretrained bert\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name)\n",
    "\n",
    "class BatchPreprocessor(object): \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        raw_sentences = []\n",
    "\n",
    "        # collect all sentences\n",
    "        for sample in batch:\n",
    "            raw_sentences.append(sample['text'])\n",
    "\n",
    "        # label processing \n",
    "        labels = []\n",
    "        for sample in batch:\n",
    "            label = sample['label']\n",
    "            labels.append(int(label))\n",
    "\n",
    "        word_ids_from_bert_tokenizer = bert_tokenizer(raw_sentences,  padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return (word_ids_from_bert_tokenizer, torch.FloatTensor(labels), raw_sentences) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First epoch data:\n",
      "input data\n",
      " {'input_ids': tensor([[  0, 118, 619,  ...,   1,   1,   1],\n",
      "        [  0, 118, 619,  ...,   1,   1,   1],\n",
      "        [  0, 118,  40,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  0, 118, 802,  ...,   1,   1,   1],\n",
      "        [  0, 118,  21,  ...,   1,   1,   1],\n",
      "        [  0, 118, 619,  ...,   1,   1,   1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "label data\n",
      " tensor([0., 0., 5., 3., 1., 0., 3., 1., 3., 1., 1., 4., 4., 0., 4., 1., 0., 0.,\n",
      "        1., 0., 2., 1., 0., 1., 0., 0., 0., 2., 3., 1., 2., 3.])\n",
      "padding mask data\n",
      " ['i feel i might have lost the potty training train', 'i feel quite helpless in all of this so prayer is the most effective tool i have because i have no answers and there is nothing else i can offer them right now', 'i will practice meditation if i feel overwhelmed and hopefully become successful in peaceful practice', 'i did not care much about the number of viewers and the viewer ratings before but as the drama iris gained huge success i began to feel greedy', 'i feel like this beats out just about any popular high end foundation on the market at either ulta or sephora', 'i feel much more energized than on a gloomy rainy autumn day', 'i realized i was feeling really irritated while i was saying that', 'i always said i felt so blessed to have him and today that feeling is been reassured many times', 'whenever i put myself in others shoes and try to make the person happy', 'im feeling optimistic to finish out these last two weeks strong and probably continue with what i have been doing', 'i feel more virtuous than when i eat veggies dipped in hummus', 'i decide that picking the easy route would get me nowhere and i feel like other people want me tortured so i follow the blue path', 'i thought i would feel apprehensive about it i was surprisingly comfortable while he was gone', 'i also know that if today i refuse to hate jews or anybody else it is because i know how it feels to be hated', 'i feel terrified because my landlord has not changed our locks yet', 'i feel benevolent enough to buy them some peanuts and other treats', 'i feel fucking woeful looking at the other girls', 'i always feel so dull in the morning', 'i feel that an input from me will be valued as being less potent than say that of irfan pathan', 'i admit im feeling a little bit unloved at this point', 'i feel a bit like a naughty kid who went and spent their last pence on a bag full of e numbers guilty', 'i plan to do so by obtaining an mba and from that mba program i feel that the most valuable outcomes i would like', 'i don t want to tag people who think this is silly but if there are people out there who want to be tagged i wouldn t want to make them feel unwelcome', 'i feel was pretty triumphant', 'i give up from my goals if i feel them boring', 'i exactly feel whenever i feel lonely or depressed and then i pray to him for help and guidance a href http', 'i hate feeling alone too', 'i feel tender when i have not done anything', 'i am feeling very petty right now', 'i thought i didnt feel anything anymore it was over it was ok well today a different story i feel him i want him my heart hurts thinking he wont be around i still want him around i guess its still valid', 'i was bonded to that point in time and still feel fond of this memory', 'i feel for vets the animals whose lives they save are always going to be hostile']\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "# dataset_example should support operator index_selection for create the data_loader object\n",
    "test_loader = DataLoader(dataset['test'], batch_size=batch_size, collate_fn=BatchPreprocessor(), shuffle=True)\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, collate_fn=BatchPreprocessor(), shuffle=True)\n",
    "valid_loader = DataLoader(dataset['validation'], batch_size=batch_size, collate_fn=BatchPreprocessor(), shuffle=True)\n",
    "for e in test_loader:\n",
    "    print('First epoch data:')\n",
    "    print('input data\\n', e[0])\n",
    "    print('label data\\n',e[1])\n",
    "    print('padding mask data\\n',e[2])\n",
    "    print(e[0]['input_ids'].device)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 500\n",
      "test size 63\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/phuongnm/deeplearning_tutorial/src/SimpleNN/3.bert_emotion-1.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspcc-a100g02/home/phuongnm/deeplearning_tutorial/src/SimpleNN/3.bert_emotion-1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(train_loader))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspcc-a100g02/home/phuongnm/deeplearning_tutorial/src/SimpleNN/3.bert_emotion-1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest size\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39mlen\u001b[39m(test_loader))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bspcc-a100g02/home/phuongnm/deeplearning_tutorial/src/SimpleNN/3.bert_emotion-1.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mlen\u001b[39m(my_vocab)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "print('train size', len(train_loader))\n",
    "print('test size',  len(test_loader))\n",
    "len(my_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "d_model = 768\n",
    "bert_model = AutoModel.from_pretrained(pre_trained_model_name)\n",
    "bert_model.cuda()\n",
    "  \n",
    "output_layer = nn.Linear(d_model, 1)\n",
    "output_layer.cuda()\n",
    "# ===================================\n",
    "\n",
    "# loss also is supported by a library \n",
    "loss_computation = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(bert_model.parameters()) + list(output_layer.parameters()), lr = 1e-5)    # using Adam optimizer instead of SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_function(bert_inputs, output_module): \n",
    "    \n",
    "    outputs = bert_model(**bert_inputs)\n",
    "    h_cls = outputs[1]\n",
    "    label_vectors = output_module(h_cls) \n",
    "\n",
    "    return label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc model BEFORE train =  tensor(0.3475, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def eval(data_loader):\n",
    "    count_true = 0\n",
    "    count_total = 0\n",
    "    for batch in data_loader:\n",
    "\n",
    "        x, y_gold, raw_sentences = batch\n",
    "\n",
    "        x = dict([(k, v.cuda()) for k, v in x.items()]) # HARD\n",
    "        y_gold = y_gold.cuda()\n",
    "        \n",
    "        # ============= ###### IMPORTANT ######## ===============\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        label_vectors = forward_function(bert_inputs=x, output_module=output_layer)\n",
    "        \n",
    "        predictions = torch.ceil(label_vectors.squeeze())\n",
    "        # ============= ######################### ===============\n",
    "        \n",
    "        count_true += torch.sum((predictions==y_gold) == True)\n",
    "        count_total += y_gold.shape[0]\n",
    "\n",
    "    return count_true / count_total\n",
    "print('Acc model BEFORE train = ', eval(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=3.7480270862579346\n",
      "step 10: loss=3.746635675430298\n",
      "step 20: loss=1.4168243408203125\n",
      "step 30: loss=1.9797720909118652\n",
      "step 40: loss=3.3791303634643555\n",
      "step 50: loss=2.530238151550293\n",
      "step 60: loss=2.7934889793395996\n",
      "step 70: loss=2.0442585945129395\n",
      "step 80: loss=1.4724265336990356\n",
      "step 90: loss=0.8465590476989746\n",
      "step 100: loss=0.8569158315658569\n",
      "step 110: loss=0.9766244888305664\n",
      "step 120: loss=0.989676833152771\n",
      "step 130: loss=0.7003448009490967\n",
      "step 140: loss=0.7205039858818054\n",
      "step 150: loss=0.7197826504707336\n",
      "step 160: loss=0.8840521574020386\n",
      "step 170: loss=0.6590648889541626\n",
      "step 180: loss=0.2490629255771637\n",
      "step 190: loss=0.5959786176681519\n",
      "step 200: loss=0.5967725515365601\n",
      "step 210: loss=0.3765519857406616\n",
      "step 220: loss=0.9442761540412903\n",
      "step 230: loss=0.5287991762161255\n",
      "step 240: loss=0.10454721748828888\n",
      "step 250: loss=0.5070899724960327\n",
      "step 260: loss=0.4888703227043152\n",
      "step 270: loss=0.5518723130226135\n",
      "step 280: loss=0.30440759658813477\n",
      "step 290: loss=0.2861593961715698\n",
      "step 300: loss=0.2516535520553589\n",
      "step 310: loss=0.3491416573524475\n",
      "step 320: loss=0.7277063727378845\n",
      "step 330: loss=0.10132155567407608\n",
      "step 340: loss=0.200880765914917\n",
      "step 350: loss=0.18789593875408173\n",
      "step 360: loss=0.32244324684143066\n",
      "step 370: loss=0.36312538385391235\n",
      "step 380: loss=0.3865496814250946\n",
      "step 390: loss=0.24363398551940918\n",
      "step 400: loss=0.7528656721115112\n",
      "step 410: loss=0.32291749119758606\n",
      "step 420: loss=0.08498971164226532\n",
      "step 430: loss=0.26384320855140686\n",
      "step 440: loss=0.45811939239501953\n",
      "step 450: loss=0.08163143694400787\n",
      "step 460: loss=0.16096609830856323\n",
      "step 470: loss=0.08387650549411774\n",
      "step 480: loss=0.051612600684165955\n",
      "step 490: loss=0.09022124111652374\n",
      "epoch/batch:  0  avg loss:  0.8536763933002949 Acc= tensor(0.4140, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MAX_EPOCHS=1\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        x, y_gold, raw_sentences = batch\n",
    "\n",
    "        x = dict([(k, v.cuda()) for k, v in x.items()]) # HARD\n",
    "        y_gold = y_gold.cuda() \n",
    "        \n",
    "\n",
    "        # ============= ###### IMPORTANT ######## ===============\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        label_vectors = forward_function(bert_inputs=x, output_module=output_layer)\n",
    "\n",
    "\n",
    "        # Compute and loss = average ((out_put - pred) ^ 2)\n",
    "        loss = loss_computation(label_vectors.squeeze(), y_gold) \n",
    "        if step %10 == 0:\n",
    "            print(f\"step {step}: loss={loss.item()}\")\n",
    "        # ============= ######################### ===============\n",
    "\n",
    "        # perform a backward pass (backpropagation) => to compute the gradient values in Tensor weights\n",
    "        loss.backward()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # USE LIBRARY: 'model.parameters()' in stead of 'model.get_parameter()' is implemented by library, also return list of parameters: \"weight\" and \"bias\" \n",
    "        # Optimizer step(), this update gradient values to weights.\n",
    "        optimizer.step() # instead of `param.add_(-lr * param.grad)` => update weight values\n",
    "        optimizer.zero_grad() # instead of `param.grad.fill_(0)` => remove all the old gradient values in all Tensor weight\n",
    "    \n",
    "    avg_loss = avg_loss / len(train_loader)\n",
    "    if avg_loss < 0.0001:\n",
    "        print(loss)\n",
    "        break\n",
    "    print('epoch/batch: ', epoch, ' avg loss: ', avg_loss, \"Acc=\", eval(valid_loader))\n",
    "    # break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc model AFTER train =  tensor(0.4030, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Acc model AFTER train = ', eval(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3602b4515afb2d87a870b61c65d7b658117eca8f37f64d20593019ba04f7019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
