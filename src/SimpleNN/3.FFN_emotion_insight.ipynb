{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model insight and analysis\n",
    "\n",
    "**Previous model:** We assum that, each word in the input sentence have a weight affect to the label. To this end, we need to learn these hidden weights. *We designed a Embedding layer (random init) to convert word to vector. Then `sum`  embedding vectors of all words in a input sentence, then transform document vector to the label values (images)*\n",
    "\n",
    "![model_arc](../../img/dl_tutorial-Trang-2.drawio.png)\n",
    "\n",
    "**Questions?** \n",
    "- If the order of words in the sentence change, the performance of this model will change or not? Why?\n",
    "- what difference thing model learn in embedding layer and the Linear output layer ?  \n",
    "- what did model learn? or which words strongly affected to the label ? For example, given sentence \"im updating my because i feel shitty\" => label is \"sadness\" , how we know that model learned the correct thing that shitty is the sad emotion? \n",
    "- Can we manually create a embedding vector for a new word (e.g. \"wa_ta_shi\") to achieve current performance without train model again? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/phuongnm/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6212d8549a144eec8577de61011a34ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    " \n",
    "# =====================\n",
    "\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import random\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"set seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_random_seed(7)\n",
    "\n",
    "def split_tokens(sentence):                             \n",
    "    return [w for w in re.split(r\" +\",  re.sub(r\"[^a-z@# ]\", \"\", sentence.lower()))]   \n",
    "\n",
    "dataset = load_dataset('emotion')\n",
    "train_data = dataset['train']\n",
    "all_words = []\n",
    "all_labels = []\n",
    "for sample in train_data:\n",
    "    all_words+= split_tokens(sample['text']) \n",
    "    all_labels.append(sample['label'])\n",
    "\n",
    "# build vocab - using vocab object of torchtext \n",
    "counter = Counter(all_words)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "my_vocab = vocab(OrderedDict(sorted_by_freq_tuples), specials=['<pad>','<unk>'])\n",
    "my_vocab.set_default_index(my_vocab['<unk>'])\n",
    "\n",
    "# count label \n",
    "num_labels = len(set(all_labels))\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def convert_sentence_to_ids(sentence, vocab):    \n",
    "    word_ids = None\n",
    "    # - split sentence to tokens using `split_tokens` defined above\n",
    "    # (tips: split sentence to list of words, then feed to the vocab to get list of id) \n",
    "    word_ids = vocab(split_tokens(sentence))\n",
    "\n",
    "    # ===================================\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "def get_max_sentence_length_in_batch(batch_input_ids): \n",
    "    # - find and return the MAXIMUM length (number of word) of each sample (sentence) in a batch.\n",
    "    \n",
    "    max_sentence_length = max([len(e) for e in batch_input_ids])\n",
    "    return max_sentence_length\n",
    "\n",
    "\n",
    "def add_padding(batch_input_ids, padding_id):\n",
    "    max_sample_len_in_batch = get_max_sentence_length_in_batch(batch_input_ids=batch_input_ids)\n",
    "\n",
    "    # - batch data contains many sentence having difference number of words. To train a deep learning model\n",
    "    #   we need to convert it to tensor which have the same length for all sentences. \n",
    "    # - We need to add padding into each sentence (sample) in a batch. \n",
    "    # - for example: a batch contains [[1,2,3,4],[6,7,8],[9]] ==(after padding 0)==> [[1,2,3,4],[6,7,8,0],[9,0,0,0]]\n",
    "    # (tips: each sample, calculate the number of padding tokens need to add to get max_sample_len_in_batch) \n",
    "    padded_word_ids = []\n",
    "    for i, word_ids in enumerate(batch_input_ids):\n",
    "        padded_word_ids.append(word_ids + [padding_id] * (max_sample_len_in_batch - len(word_ids)))\n",
    "    return padded_word_ids\n",
    "\n",
    "\n",
    "class BatchPreprocessor(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs = []\n",
    "        masks = []\n",
    "        raw_sentences = []\n",
    "\n",
    "        # covert text to number \n",
    "        for sample in batch:\n",
    "            word_ids = convert_sentence_to_ids(sample['text'], self.vocab)\n",
    "            inputs.append(word_ids)\n",
    "            raw_sentences.append(split_tokens(sample['text']))\n",
    "        \n",
    "        # padding to create a tensor input - make all sentence having the same length \n",
    "        padding_id = self.vocab[\"<pad>\"]\n",
    "        padded_batch = add_padding(batch_input_ids=inputs, padding_id=padding_id)\n",
    "\n",
    "        # label processing \n",
    "        labels = []\n",
    "        for sample in batch:\n",
    "            label = sample['label']\n",
    "            labels.append(int(label))\n",
    "\n",
    "        # make a tensor \n",
    "        inputs = torch.LongTensor(padded_batch)\n",
    "\n",
    "        # make mask flag tensor\n",
    "        masks = inputs == padding_id\n",
    "\n",
    "        return (inputs, torch.FloatTensor(labels), torch.BoolTensor(masks), raw_sentences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First epoch data:\n",
      "input data\n",
      " tensor([[   2,    3,    2,  ...,    0,    0,    0],\n",
      "        [   2,    3,  157,  ...,    0,    0,    0],\n",
      "        [   2,   59, 1341,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,   40,  604,  ...,    0,    0,    0],\n",
      "        [   2, 1293,    6,  ...,    0,    0,    0],\n",
      "        [   2,    3,   38,  ...,    0,    0,    0]])\n",
      "label data\n",
      " tensor([0., 0., 5., 3., 1., 0., 3., 1., 3., 1., 1., 4., 4., 0., 4., 1., 0., 0.,\n",
      "        1., 0., 2., 1., 0., 1., 0., 0., 0., 2., 3., 1., 2., 3., 0., 5., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 4., 1., 4., 4., 1., 0., 0., 1., 4., 0., 2., 1.,\n",
      "        0., 1., 1., 1., 2., 3.])\n",
      "padding mask data\n",
      " tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 60\n",
    "\n",
    "# dataset_example should support operator index_selection for create the data_loader object\n",
    "test_loader = DataLoader(dataset['test'], batch_size=batch_size, collate_fn=BatchPreprocessor(my_vocab), shuffle=True)\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, collate_fn=BatchPreprocessor(my_vocab), shuffle=True)\n",
    "valid_loader = DataLoader(dataset['validation'], batch_size=batch_size, collate_fn=BatchPreprocessor(my_vocab), shuffle=True)\n",
    "for e in test_loader:\n",
    "    print('First epoch data:')\n",
    "    print('input data\\n', e[0])\n",
    "    print('label data\\n',e[1])\n",
    "    print('padding mask data\\n',e[2])\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 267\n",
      "test size 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15214"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('train size', len(train_loader))\n",
    "print('test size',  len(test_loader))\n",
    "len(my_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "d_model = 200\n",
    "word_embedding = nn.Embedding(num_embeddings=len(my_vocab), embedding_dim=d_model, padding_idx=my_vocab['<pad>'])\n",
    "word_embedding.cuda()\n",
    "\n",
    "# ===================================\n",
    "# REQUIREMENT:\n",
    "# - construct a Linear (dense connection) layer to transform a document vector (embedding size) to 1 \n",
    "# - NOTE: then move this layer to CUDA device for computation \n",
    "# ===================================\n",
    "# - PUSH YOUR CODE IN HERE, can not modify any code in outside this range.  \n",
    "\n",
    "output_layer = nn.Linear(d_model, 1)\n",
    "output_layer.cuda()\n",
    "# ===================================\n",
    "\n",
    "# loss also is supported by a library \n",
    "loss_computation = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(word_embedding.parameters()) + list(output_layer.parameters()), lr = 1e-3)    # using Adam optimizer instead of SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_function(w_vectors, output_module):\n",
    "    # ===================================\n",
    "    # REQUIREMENT: compute emotion prediction values given all word embedding vectors \n",
    "    # we assum that, each word in the input sentence have a weight effect to the label\n",
    "    # and we need to learn these hidden weights.\n",
    "    #\n",
    "    # - Compute the document vector based on the input word embedding vector based on sum operator. \n",
    "    #   e.g. doc1 = sum([w1, w2, ... wn]) = w1 + w2 + ... + wn \n",
    "    #   NOTE: check function `torch.sum` (https://pytorch.org/docs/stable/generated/torch.sum.html)\n",
    "    #\n",
    "    # - forward the document vector to `output_module` layer to get emotion values. \n",
    "    # ===================================\n",
    "    # - PUSH YOUR CODE IN HERE, can not modify any code in outside this range. \n",
    "    \n",
    "    doc_vectors = torch.sum(w_vectors, dim=1)\n",
    "    label_vectors = output_layer(doc_vectors)\n",
    "    # ===================================\n",
    "    return label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc model BEFORE train =  tensor(0.0640, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def eval(data_loader):\n",
    "    count_true = 0\n",
    "    count_total = 0\n",
    "    for batch in data_loader:\n",
    "\n",
    "        x, y_gold, masked, raw_sentences = batch\n",
    "\n",
    "        x = x.cuda()\n",
    "        y_gold = y_gold.cuda()\n",
    "        \n",
    "        # ============= ###### IMPORTANT ######## ===============\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        w_vectors = word_embedding(x) #  batch size x sequence length x hidden size \n",
    "        label_vectors = forward_function(w_vectors=w_vectors, output_module=output_layer)\n",
    "        \n",
    "        predictions = torch.ceil(label_vectors.squeeze())\n",
    "        # ============= ######################### ===============\n",
    "\n",
    "        count_true += torch.sum((predictions==y_gold) == True)\n",
    "        count_total += x.shape[0]\n",
    "\n",
    "    return count_true / count_total\n",
    "print('Acc model BEFORE train = ', eval(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/batch:  0  avg loss:  4.21558777446604 Acc= tensor(0.2355, device='cuda:0')\n",
      "epoch/batch:  1  avg loss:  1.9817629467681999 Acc= tensor(0.2235, device='cuda:0')\n",
      "epoch/batch:  2  avg loss:  1.3952480605925515 Acc= tensor(0.3245, device='cuda:0')\n",
      "epoch/batch:  3  avg loss:  0.9620905409144999 Acc= tensor(0.3225, device='cuda:0')\n",
      "epoch/batch:  4  avg loss:  0.7260113397564334 Acc= tensor(0.3395, device='cuda:0')\n",
      "epoch/batch:  5  avg loss:  0.5901127916373564 Acc= tensor(0.3675, device='cuda:0')\n",
      "epoch/batch:  6  avg loss:  0.5063367780116613 Acc= tensor(0.3905, device='cuda:0')\n",
      "epoch/batch:  7  avg loss:  0.45053471077917223 Acc= tensor(0.3860, device='cuda:0')\n",
      "epoch/batch:  8  avg loss:  0.4064252081881748 Acc= tensor(0.4380, device='cuda:0')\n",
      "epoch/batch:  9  avg loss:  0.3780720525801405 Acc= tensor(0.4205, device='cuda:0')\n",
      "epoch/batch:  10  avg loss:  0.3536578690291344 Acc= tensor(0.4490, device='cuda:0')\n",
      "epoch/batch:  11  avg loss:  0.3337717227051767 Acc= tensor(0.4185, device='cuda:0')\n",
      "epoch/batch:  12  avg loss:  0.318632663272963 Acc= tensor(0.4135, device='cuda:0')\n",
      "epoch/batch:  13  avg loss:  0.30866369719474057 Acc= tensor(0.3755, device='cuda:0')\n",
      "epoch/batch:  14  avg loss:  0.29818107508653113 Acc= tensor(0.3940, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MAX_EPOCHS=15\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "\n",
    "        x, y_gold, masked, raw_sentences = batch\n",
    "\n",
    "        x = x.cuda()\n",
    "        y_gold = y_gold.cuda() \n",
    "        \n",
    "\n",
    "        # ============= ###### IMPORTANT ######## ===============\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        w_vectors = word_embedding(x) #  batch size x sequence length x hidden size  \n",
    "\n",
    "        label_vectors = forward_function(w_vectors=w_vectors, output_module=output_layer)\n",
    "\n",
    "        # Compute and loss = average ((out_put - pred) ^ 2)\n",
    "        loss = loss_computation(label_vectors.squeeze(), y_gold) \n",
    "        # ============= ######################### ===============\n",
    "\n",
    "        # perform a backward pass (backpropagation) => to compute the gradient values in Tensor weights\n",
    "        loss.backward()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # USE LIBRARY: 'model.parameters()' in stead of 'model.get_parameter()' is implemented by library, also return list of parameters: \"weight\" and \"bias\" \n",
    "        # Optimizer step(), this update gradient values to weights.\n",
    "        optimizer.step() # instead of `param.add_(-lr * param.grad)` => update weight values\n",
    "        optimizer.zero_grad() # instead of `param.grad.fill_(0)` => remove all the old gradient values in all Tensor weight\n",
    "    \n",
    "    avg_loss = avg_loss / len(train_loader)\n",
    "    if avg_loss < 0.0001:\n",
    "        print(loss)\n",
    "        break\n",
    "    print('epoch/batch: ', epoch, ' avg loss: ', avg_loss, \"Acc=\", eval(valid_loader))\n",
    "    # break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc model AFTER train =  tensor(0.3895, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Acc model AFTER train = ', eval(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_list= ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
      "\n",
      "====================================================================================================\n",
      "org input ids tensor([[  17,    8,  203,  715,   15,   17,   26,   46, 5595,  114,   58]])\n",
      "org raw [['im', 'feeling', 'rather', 'rotten', 'so', 'im', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.]) sadness\n",
      "label vect tensor([[-0.2333]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "----\n",
      "word is removed =>  im\n",
      "new input ids tensor([[   8,  203,  715,   15,   17,   26,   46, 5595,  114,   58]])\n",
      "[['feeling', 'rather', 'rotten', 'so', 'im', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.3074]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  feeling\n",
      "new input ids tensor([[  17,  203,  715,   15,   17,   26,   46, 5595,  114,   58]])\n",
      "[['im', 'rather', 'rotten', 'so', 'im', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.5811]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  rather\n",
      "new input ids tensor([[  17,    8,  715,   15,   17,   26,   46, 5595,  114,   58]])\n",
      "[['im', 'feeling', 'rotten', 'so', 'im', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.3374]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  rotten\n",
      "new input ids tensor([[  17,    8,  203,   15,   17,   26,   46, 5595,  114,   58]])\n",
      "[['im', 'feeling', 'rather', 'so', 'im', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[0.8245]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(1., device='cuda:0', grad_fn=<CeilBackward0>) joy\n",
      "\n",
      "\n",
      "word is removed =>  so\n",
      "new input ids tensor([[  17,    8,  203,  715,   17,   26,   46, 5595,  114,   58]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'im', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.1873]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  im\n",
      "new input ids tensor([[  17,    8,  203,  715,   15,   26,   46, 5595,  114,   58]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'so', 'not', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.3074]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  not\n",
      "new input ids tensor([[  17,    8,  203,  715,   15,   17,   46, 5595,  114,   58]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'so', 'im', 'very', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.2576]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  very\n",
      "new input ids tensor([[  17,    8,  203,  715,   15,   17,   26, 5595,  114,   58]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'so', 'im', 'not', 'ambitious', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.2655]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  ambitious\n",
      "new input ids tensor([[ 17,   8, 203, 715,  15,  17,  26,  46, 114,  58]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'so', 'im', 'not', 'very', 'right', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[0.3426]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(1., device='cuda:0', grad_fn=<CeilBackward0>) joy\n",
      "\n",
      "\n",
      "word is removed =>  right\n",
      "new input ids tensor([[  17,    8,  203,  715,   15,   17,   26,   46, 5595,   58]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'so', 'im', 'not', 'very', 'ambitious', 'now']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.1776]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  now\n",
      "new input ids tensor([[  17,    8,  203,  715,   15,   17,   26,   46, 5595,  114]])\n",
      "[['im', 'feeling', 'rather', 'rotten', 'so', 'im', 'not', 'very', 'ambitious', 'right']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.2058]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "org input ids tensor([[   17, 12800,    11,   243,    37,     2,     3,   469]])\n",
      "org raw [['im', 'updating', 'my', 'blog', 'because', 'i', 'feel', 'shitty']]\n",
      "label gold tensor([0.]) sadness\n",
      "label vect tensor([[-0.4340]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "----\n",
      "word is removed =>  im\n",
      "new input ids tensor([[12800,    11,   243,    37,     2,     3,   469]])\n",
      "[['updating', 'my', 'blog', 'because', 'i', 'feel', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.5081]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  updating\n",
      "new input ids tensor([[ 17,  11, 243,  37,   2,   3, 469]])\n",
      "[['im', 'my', 'blog', 'because', 'i', 'feel', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[0.2141]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(1., device='cuda:0', grad_fn=<CeilBackward0>) joy\n",
      "\n",
      "\n",
      "word is removed =>  my\n",
      "new input ids tensor([[   17, 12800,   243,    37,     2,     3,   469]])\n",
      "[['im', 'updating', 'blog', 'because', 'i', 'feel', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.4522]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  blog\n",
      "new input ids tensor([[   17, 12800,    11,    37,     2,     3,   469]])\n",
      "[['im', 'updating', 'my', 'because', 'i', 'feel', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.3762]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  because\n",
      "new input ids tensor([[   17, 12800,    11,   243,     2,     3,   469]])\n",
      "[['im', 'updating', 'my', 'blog', 'i', 'feel', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.3440]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  i\n",
      "new input ids tensor([[   17, 12800,    11,   243,    37,     3,   469]])\n",
      "[['im', 'updating', 'my', 'blog', 'because', 'feel', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.5019]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  feel\n",
      "new input ids tensor([[   17, 12800,    11,   243,    37,     2,   469]])\n",
      "[['im', 'updating', 'my', 'blog', 'because', 'i', 'shitty']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[-0.7914]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(-0., device='cuda:0', grad_fn=<CeilBackward0>) sadness\n",
      "\n",
      "\n",
      "word is removed =>  shitty\n",
      "new input ids tensor([[   17, 12800,    11,   243,    37,     2,     3]])\n",
      "[['im', 'updating', 'my', 'blog', 'because', 'i', 'feel']]\n",
      "label gold tensor([0.], device='cuda:0') sadness\n",
      "label vect tensor([[0.5945]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "label pred tensor(1., device='cuda:0', grad_fn=<CeilBackward0>) joy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_list = ['sadness' , 'joy'  , 'love'  , 'anger' , 'fear'  , 'surprise'  ]  \n",
    "print('label_list=', label_list)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=1, collate_fn=BatchPreprocessor(my_vocab), shuffle=False)\n",
    "for j, batch in enumerate(test_loader):\n",
    "\n",
    "    print(\"\\n\"+\"\".join([\"=\"]*100))\n",
    "    \n",
    "    # ============= ###### ORIGINAL prediction ######## ===============\n",
    "    x, y_gold, masked, raw_sentences = batch\n",
    "    print('org input ids', x)\n",
    "    print('org raw',raw_sentences)\n",
    "    print('label gold',y_gold,label_list[int(y_gold.item())])\n",
    "    x_new = x.cuda()\n",
    "    y_gold = y_gold.cuda()\n",
    "    w_vectors = word_embedding(x_new) #  batch size x sequence length x hidden size \n",
    "    label_vectors = forward_function(w_vectors=w_vectors, output_module=output_layer)\n",
    "    predictions = torch.ceil(label_vectors.squeeze())\n",
    "    print('label vect', label_vectors)\n",
    "    print('label pred',predictions, label_list[int(predictions.item())])\n",
    "    print('----')\n",
    "    # ============= ######################### ===============\n",
    "\n",
    "    # ============= ###### temporary remove WORD by WORD to check new prediction ######## ===============\n",
    "    for i in range(len(raw_sentences[0])):\n",
    "        x_new = torch.cat((x[:, :i], x[:, i+1:]), dim=1)\n",
    "        new_raw_sentences = [raw_sentences[0][:i]+raw_sentences[0][i+1:]]\n",
    "\n",
    "        print('word is removed => ', raw_sentences[0][i])\n",
    "        print('new input ids', x_new)\n",
    "        print(new_raw_sentences)\n",
    "        print('label gold',y_gold,label_list[int(y_gold.item())])\n",
    "        \n",
    "        x_new = x_new.cuda()\n",
    "        y_gold = y_gold.cuda()\n",
    "         \n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        w_vectors = word_embedding(x_new) #  batch size x sequence length x hidden size \n",
    "        label_vectors = forward_function(w_vectors=w_vectors, output_module=output_layer)\n",
    "        print('label vect', label_vectors)\n",
    "        \n",
    "        predictions = torch.ceil(label_vectors.squeeze())\n",
    "        print('label pred',predictions, label_list[int(predictions.item())])\n",
    "\n",
    "        print(\"\\n\")\n",
    "    # ============= ######################### ===============\n",
    "    \n",
    "    if j ==1: break \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3602b4515afb2d87a870b61c65d7b658117eca8f37f64d20593019ba04f7019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
